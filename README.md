# Awesome-PEFT
### Papers
|Year|Paper Title|Proposed Method|Venue|Materials|
|------------------------------|------------------------------|------------------------------|------------------------------|-----------------------------------------------|
|2019|Parameter-Efficient Transfer Learning for NLP|Adapter tuning|ICML|[[PUB](https://proceedings.mlr.press/v97/houlsby19a.html)][[CODE](https://github.com/google-research/adapter-bert)]|
|2019|Simple, Scalable Adaptation for Neural Machine Translation|Single Adapter|EMNLP|[[PUB](https://aclanthology.org/D19-1165/)]|
|2020|Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning|VLM|EMNLP Findings|[[PUB](https://aclanthology.org/2020.findings-emnlp.41/)]|
|2021|AdapterFusion: Non-destructive task composition for transfer learning|AdapterFusion|EACL|[[PUB](https://aclanthology.org/2021.eacl-main.39/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter)]|
|2021|Parameter-Efficient Transfer Learning with Diff Pruning|DiffPruning|ACL|[[PUB](https://aclanthology.org/2021.acl-long.378/)]|
|2021|Prefix-Tuning: Optimizing Continuous Prompts for Generation|Prefix tuning|ACL|[[PDF](https://xiangli1999.github.io/pdf/prefix_tuning.pdf)][[CODE](https://github.com/XiangLi1999/PrefixTuning)]|
|2021|Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning|Intrinsic SAID|ACL|[[PDF](https://aclanthology.org/2021.acl-long.568.pdf)]|
|2021|AdapterDrop: On the efficiency of adapters in transformers|AdapterDrop|EMNLP|[[PUB](https://aclanthology.org/2021.emnlp-main.626/)]|
|2021|The Power of Scale for Parameter-Efficient Prompt Tuning| Prompt tuning|EMNLP|[[PUB](https://aclanthology.org/2021.emnlp-main.243/)]|
|2021|Compacter: Efficient low-rank hypercomplex adapter layers|Compacter|NeurIPS|[[PDF](https://proceedings.neurips.cc/paper/2021/file/081be9fdff07f3bc808f935906ef70c0-Paper.pdf)][[CODE](https://github.com/rabeehk/compacter)]|
|2021|Training Neural Networks with Fixed Sparse Masks|Fish-Mask|NeurIPS|[[PDF](https://proceedings.neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf)][[CODE](https://github.com/varunnair18/FISH)]|
|2022|Towards a Unified View of Parameter-Efficient Transfer Learning|MAM Adapter, ParallelAdapter|ICLR|[[PDF](https://xuezhemax.github.io/assets/publications/pdfs/iclr2022_towards.pdf)][[CODE](https://github.com/jxhe/unify-parameter-efficient-tuning)]|
|2022|UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning|UniPELT|ACL|[[PUB](https://aclanthology.org/2022.acl-long.433/)]|
|2022|P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks|P-tuning v2|ACL|[[PUB](https://aclanthology.org/2022.acl-short.8/)]|
|2022|BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models|BitFit|ACL|[[PUB](https://aclanthology.org/2022.acl-short.1/)]|
|2022|LoRA: Low-Rank Adaptation of Large Language Models|LoRA|ICLR|[[PDF](https://openreview.net/pdf?id=nZeVKeeFYf9)][[CODE](https://github.com/microsoft/LoRA)]|
|2022|Efficient Fine-Tuning of BERT Models on the Edge|FAR|ISCAS|[[PUB](https://ieeexplore.ieee.org/abstract/document/9937567)][[PDF](https://arxiv.org/pdf/2205.01541)]|
|2022|Adamix: Mixture-of adapter for parameter-efficient tuning of large language models|Adamix|EMNLP|[[PDF](https://www.microsoft.com/en-us/research/uploads/prod/2022/05/Mixture_of_Adapters-628fa6a57efd3.pdf)][[CODE](https://github.com/microsoft/AdaMix)]|
|2022|SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters|SparseAdapter|EMNLP Findings|[[PUB](https://aclanthology.org/2022.findings-emnlp.160/)][[CODE](https://github.com/Shwai-He/SparseAdapter)]|
|2022|Exploring universal intrinsic task subspace via prompt tuning|IPT|EMNLP Finding|[[PDF](https://arxiv.org/pdf/2110.07867)][[CODE](https://github.com/thunlp/Intrinsic-Prompt-Tuning)]|
|2022|Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning|(IA)<SUP>3<SUP/>|NeurIPS|[[PUB](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html)][[CODE](https://github.com/r-three/t-few)]|
|2022|LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning|LST|NeurIPS|[[PDF](https://proceedings.neurips.cc/paper_files/paper/2022/file/54801e196796134a2b0ae5e8adef502f-Paper-Conference.pdf)]|
|2023|Sparse Low-rank Adaptation of Pre-trained Language Models|SoRA|EMNLP|[[PDF](https://aclanthology.org/2023.emnlp-main.252.pdf)]|
|2023|DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation|DyLoRA|EACL|[[PDF](https://aclanthology.org/2023.eacl-main.239.pdf)][[CODE](https://github.com/huawei-noah/Efficient-NLP/tree/main/DyLoRA)]|
|2023|Adaptive budget allocation for parameter-efficient fine-tuning|AdaLoRA|ICLR|[[PDF](https://par.nsf.gov/servlets/purl/10471451)][[CODE](https://github.com/QingruZhang/AdaLoRA)]|
|2023|Parameter-efficient fine-tuning design spaces|S4|ICLR|[[PDF](https://openreview.net/pdf?id=XSRSWxyJIC)]|
|2023|Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning|MPT|ICLR|[[PUB](https://zhenwang9102.github.io/mpt.html)]|
|2023|QLoRA: Efficient Finetuning of Quantized LLMs|QLoRA|NeurIPS|[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html)][[CODE](https://github.com/artidoro/qlora)]|
|2023|Composing Parameter-Efficient Modules with Arithmetic Operation|ControlPE|NeurIPS|[[PDF](https://openreview.net/pdf?id=5r3e27I9Gy)]|
|2023|Controlling Text-to-Image Diffusion by Orthogonal Finetuning|OFT|NeurIPS|[[PUB](https://oft.wyliu.com/)]|
|2023|Krona: Parameter efficient tuning with kronecker adapter|KronA|NeurIPS Workshop|[[PDF](https://neurips2023-enlsp.github.io/papers/paper_61.pdf)]|
|2023|Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization|Token-Level Adaptation|AICCC|[[PDF](https://dl.acm.org/doi/pdf/10.1145/3639592.3639615)]|
|2023|Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning|SaLoRA|Mathematics|[[PDF](https://mdpi-res.com/d_attachment/mathematics/mathematics-11-04317/article_deploy/mathematics-11-04317.pdf?version=1697536399)]|
|2023|PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching|PILLOW|EMNLP Industry Track|[[PDF](https://aclanthology.org/2023.emnlp-industry.45.pdf)]|
|2023|Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices|Delta-LoRA|arXiv|[[PDF](https://arxiv.org/pdf/2309.02411)]|
|2023|LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning|LoRA-FA|arXiv|[[PDF](https://arxiv.org/pdf/2308.03303)]|
|2023|A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA|rsLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2312.03732)]|
|2023|IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning|IncreLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2308.12043)][[CODE](https://github.com/FeiyuZhang98/IncreLoRA)]|
|2024|MEFT:Memory-Efficient Fine-Tuning through Sparse Adapter|MEFT|ACL|[[PDF](https://arxiv.org/pdf/2406.04984)][[CODE](https://github.com/CURRENTF/MEFT)]|
|2024|Multimodal Instruction Tuning with Conditional Mixture of LoRA|MixLoRA|ACL|[[PDF](https://aclanthology.org/2024.acl-long.38.pdf)]|
|2024|DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution|DoRA|ACL|[[PDF](https://aclanthology.org/2024.acl-long.626.pdf)][[CODE](https://github.com/MIkumikumi0116/DoRA)]|
|2024|MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning|MELoRA|ACL|[[PDF](https://aclanthology.org/2024.acl-long.168.pdf)]|
|2024|AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models|AFLoRA|ACL|[[PDF](https://aclanthology.org/2024.acl-short.16.pdf)]|
|2024|ResLoRA: Identity Residual Mapping in Low-Rank Adaption|ResLoRA|ACL Finding|[[PDF](https://aclanthology.org/2024.findings-acl.525.pdf)][[CODE](https://github.com/microsoft/LMOps/tree/main/reslora)]|
|2024|SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning|SIBO|ACL Finding|[[PDF](https://aclanthology.org/2024.findings-acl.72.pdf)]|
|2024|STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models|STAR|ACL Finding|[[PDF](https://aclanthology.org/2024.findings-acl.209.pdf)]|
|2024|LoRA Meets Dropout under a Unified Framework|HiddenKey|ACL Finding|[[PDF](https://aclanthology.org/2024.findings-acl.119.pdf)]|
|2024|LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning|LoRAPrune|ACL Finding|[[PDF](https://aclanthology.org/2024.findings-acl.178.pdf)][[CODE](https://github.com/aim-uofa/LoRAPrune)]|
|2024|GPT Understands, Too|P-tuning|AI Open|[[PUB](https://www.sciencedirect.com/science/article/pii/S2666651023000141)]|
|2024|VeRA: Vector-based Random Matrix Adaptation|VeRA|ICLR|[[PUB](https://dkopi.github.io/vera/)]|
|2024|Parameter-Efficient Multi-Task Model Fusion with Partial Linearization|L-LoRA|ICLR|[[PDF](https://openreview.net/pdf?id=iynRvVVAmH)]|
|2024|Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning|LN tuning|ICLR|[[PDF](https://openreview.net/pdf?id=YR3ETaElNK)]|
|2024|Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization|BOFT|ICLR|[[PUB](https://boft.wyliu.com/)]|
|2024|Bayesian Low-rank Adaptation for Large Language Models|Laplace-LoRA|ICLR|[[PDF](https://openreview.net/pdf?id=FJiUyzOF1m)]|
|2024|Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation|LyCORIS|ICLR|[[PDF](https://arxiv.org/pdf/2309.14859)][[CODE](https://github.com/KohakuBlueleaf/LyCORIS)]|
|2024|QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models|QA-LoRA|ICLR|[[PDF](https://openreview.net/pdf?id=WvFoJccpo8)][[CODE](https://github.com/yuhuixu1993/qa-lora)]|
|2024|LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models|LoftQ|ICLR|[[PDF](https://openreview.net/pdf?id=LzPWWPAdY4)][[CODE](https://github.com/yxli2123/LoftQ)]|
|2024|ReLoRA: High-Rank Training Through Low-Rank Updates|ReLoRA|ICLR|[[PDF](https://openreview.net/pdf?id=DLJznSp6X3)][[CODE](https://github.com/guitaricet/relora)]|
|2024|ApiQ: Finetuning of 2-Bit Quantized Large Language Model|ApiQ|EMNLP|[[PDF](https://aclanthology.org/2024.emnlp-main.1168.pdf)][[CODE](https://github.com/BaohaoLiao/ApiQ)]|
|2024|DoRA: Weight-Decomposed Low-Rank Adaptation|DoRA|ICML|[[PDF](https://openreview.net/pdf?id=3d5CIRG1n2)][[CODE](https://github.com/NVlabs/DoRA)]|
|2024|RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation|RoSA|ICML|[[PDF](https://openreview.net/pdf?id=FYvpxyS43U)][[CODE](https://github.com/IST-DASLab/RoSA)]|
|2024|LoRA+: Efficient Low Rank Adaptation of Large Models|LoRA+|ICML|[[PDF](https://openreview.net/pdf?id=NEv8YqBROO)][[CODE](https://github.com/nikhil-ghosh-berkeley/loraplus)]|
|2024|Parameter-Efficient Fine-Tuning with Discrete Fourier Transform|FourierFT|ICML|[[PDF](https://arxiv.org/pdf/2405.03003)][[CODE](https://github.com/Chaos96/fourierft)]|
|2024|Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning|COLA|ICML|[[PDF](https://arxiv.org/pdf/2401.04151)]|
|2024|HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning|HydraLoRA|NeurIPS|[[PDF](https://openreview.net/pdf?id=qEpi8uWX3N)][[CODE](https://github.com/Clin0212/HydraLoRA)]|
|2024|Sparse High Rank Adapters|SHiRA|NeurIPS|[[PDF](https://arxiv.org/pdf/2406.13175)]|
|2024|S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity|S2FT|NeurIPS|[[PUB](https://infini-ai-lab.github.io/S2FT-Page/)]|
|2024|LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning|LISA|NeurIPS|[[PDF](https://proceedings.neurips.cc/paper_files/paper/2024/file/687163285b8affc8ee933bdca8e75747-Paper-Conference.pdf)]|
|2024|DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation|DropBP|NeurIPS|[[PDF](https://openreview.net/pdf?id=x4EoTQW7ka)][[CODE](https://github.com/WooSunghyeon/dropbp)]|
|2024|VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks|VB-LoRA|NeurIPS|[[PDF](https://arxiv.org/pdf/2405.15179)][[CODE](https://github.com/leo-yangli/VB-LoRA)]|
|2024|Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation|HRA|NeurIPS|[[PDF](https://arxiv.org/pdf/2405.17484)][[CODE](https://github.com/DaShenZi721/HRA)]|
|2024|PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models|PiSSA|NeurIPS|[[PDF](https://arxiv.org/pdf/2404.02948)][[CODE](https://github.com/GraphPKU/PiSSA)]|
|2024|CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning|CorDA|NeurIPS|[[PDF](https://openreview.net/pdf?id=Gi00NVru6n)][[CODE](https://github.com/iboing/CorDA)]|
|2024|ReFT: Representation Finetuning for Language Models|ReFT|NeurIPS|[[PDF](https://arxiv.org/pdf/2404.03592)][[CODE](https://github.com/stanfordnlp/pyreft)]|
|2024|BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models|BLoB|NeurIPS|[[PDF](https://arxiv.org/pdf/2406.11675)][[CODE](https://arxiv.org/pdf/2406.11675)]|
|2024|LoRA-GA: Low-Rank Adaptation with Gradient Approximation|LoRA-GA|NeurIPS|[[PDF](https://proceedings.neurips.cc/paper_files/paper/2024/file/62c4718cc334f6a0a62fb81c4a2095a1-Paper-Conference.pdf)][[CODE](https://github.com/Outsider565/LoRA-GA)]|
|2024|One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation|EVA|NeurIPS Workshop|[[PDF](https://openreview.net/pdf?id=X6AOzi82oo)]|
|2024|SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors|SVFT|NeurIPS Workshop|[[PDF](https://openreview.net/pdf?id=DOUskwCqg5)]|
|2024|ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models|ALoRA|NAACL|[[PDF](https://aclanthology.org/2024.naacl-long.35.pdf)]|
|2024|AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning|AutoLoRA|NAACL|[[PDF](https://aclanthology.org/2024.naacl-long.282.pdf)][[CODE](https://github.com/ruz048/AutoLoRA)]|
|2024|X-LoRA: Mixture of low-rank adapter experts, a flexible framework for large language models with applications in protein mechanics and molecular design|X-LoRA|APL Machine Learning|[[PUB](https://pubs.aip.org/aip/aml/article/2/2/026119/3294581/X-LoRA-Mixture-of-low-rank-adapter-experts-a)][[PDF](https://arxiv.org/pdf/2402.07148)]|
|2024|LoRA-SP: streamlined partial parameter adaptation for resource efficient fine-tuning of large language models|LoRA-SP|AMNA|[[PUB](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13171/131711Z/LoRA-SP--streamlined-partial-parameter-adaptation-for-resource-efficient/10.1117/12.3032013.short)]|
|2024|S-LoRA: Serving Thousands of Concurrent LoRA Adapters|S-LoRA|MLSys|[[PDF](https://arxiv.org/pdf/2311.03285)][[CODE](https://github.com/S-LoRA/S-LoRA)]|
|2024|Punica: Multi-Tenant LoRA Serving|Punica|MLSys|[[PDF](https://proceedings.mlsys.org/paper_files/paper/2024/file/054de805fcceb78a201f5e9d53c85908-Paper-Conference.pdf)][[CODE](https://github.com/punica-ai/punica)]|
|2024|BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models|BiLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2403.13037)]|
|2024|LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters|LoRA-XS|arXiv|[[PDF](https://arxiv.org/pdf/2405.17604)][[CODE](https://github.com/MohammadrezaBanaei/LoRA-XS)]|
|2024|CaraServe: CPU-Assisted and Rank-Aware LoRA Serving for Generative LLM Inference|CaraServe|arXiv|[[PDF](https://arxiv.org/pdf/2401.11240)]|
|2024|L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models|L4Q|arXiv|[[PDF](https://arxiv.org/pdf/2402.04902)]|
|2024|ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization|ComPEFT|arXiv|[[PDF](https://arxiv.org/pdf/2311.13171)][[CODE](https://github.com/prateeky2806/compeft)]|
|2025|MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning|MiLoRA|NAACL|[[PDF](https://arxiv.org/pdf/2406.09044)][[CODE](https://github.com/sufenlp/MiLoRA)]|
|2025|HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models|HiRA|ICLR|[[PDF](https://openreview.net/pdf?id=TwJrTz9cRS)][[CODE](https://github.com/hqsiswiliam/hira)]|
|2025|LoRA-Pro: Are Low-Rank Adapters Properly Optimized?|LoRA-Pro|ICLR|[[PDF](https://openreview.net/pdf?id=gTwRMU3lJ5)]|
|2025|LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization|LoRA-RITE|ICLR|[[PDF](https://openreview.net/pdf?id=VpWki1v2P8)]|
|2025|ComLoRA: A Competitive Learning Approach for Enhancing LoRA|ComLoRA|ICLR|[[PDF](https://openreview.net/pdf?id=jFcNXJGPGh)]|
|2025|RaSA: Rank-Sharing Low-Rank Adaptation|RaSA|ICLR|[[PDF](https://openreview.net/pdf?id=GdXI5zCoAt)][[CODE](https://github.com/zwhe99/RaSA)]|
|2025|Quantum-PEFT: Ultra parameter-efficient fine-tuning|Quantum-PEFT|ICLR|[[PDF](https://openreview.net/pdf?id=dgR6i4TSng)]|
|2025|SMT: Fine-Tuning Large Language Models with Sparse Matrices|SMT|ICLR|[[PDF](https://openreview.net/pdf?id=GbgCRJedQ7)]|
|2025|RandLoRA: Full rank parameter-efficient fine-tuning of large models|RandLoRA|ICLR|[[PDF](https://openreview.net/pdf?id=Hn5eoTunHN)]|
|2025|SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation|SaLoRA|ICLR|[[PDF](https://openreview.net/pdf?id=GOoVzE9nSj)][[CODE](https://github.com/homles11/SaLoRA)]|
|2025|PaCA: Partial Connection Adaptation for Efficient Fine-Tuning|PaCA|ICLR|[[PDF](https://openreview.net/pdf?id=iYkhxre0In)][[CODE](https://github.com/WooSunghyeon/paca)]|
|2025|HMoRA: Making LLMs More Effective with Hierarchical Mixture of LoRA Experts|HMoRA|ICLR|[[PDF](https://openreview.net/pdf?id=lTkHiXeuDl)][[CODE](https://github.com/LiaoMengqi/HMoRA)]|
|2025|LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning|LoCA|ICLR|[[PDF](https://openreview.net/pdf?id=4NRjdISWby)]|
|2025|MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models|MeteoRA|ICLR|[[PDF](https://openreview.net/pdf?id=yOOJwR15xg)]|
|2025|Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering|LoRA-LEGO|ICLR|[[PDF](https://openreview.net/pdf?id=j6fsbpAllN)]|
|2025|KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models|KaSA|ICLR|[[PDF](https://openreview.net/pdf?id=OQqNieeivq)][[CODE](https://github.com/juyongjiang/KaSA)]|
|2025|GeoLoRA: Geometric integration for parameter efficient fine-tuning|GeoLoRA|ICLR|[[PDF](https://openreview.net/pdf?id=bsFWJ0Kget)][[CODE](https://github.com/ScSteffen/Publication-GeoLoRA-Geometric-integration-for-parameter-efficient-fine-tuning)]|
|2025|SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning|SD-LoRA|ICLR|[[PDF](https://openreview.net/pdf?id=5U1rlpX68A)][[CODE](https://github.com/WuYichen-97/SD-Lora-CL)]|
|2025|RandLoRA: Full-rank parameter-efficient fine-tuning of large models|RandLoRA|ICLR|[[PDF](https://arxiv.org/pdf/2502.00987)][[CODE](https://github.com/PaulAlbert31/RandLoRA)]|
|2025|BeamLoRA: Beam-Constraint Low-Rank Adaptation|BeamLoRA|ACL|[[PDF](https://arxiv.org/pdf/2502.13604)]|
|2025|Parameter-Efficient Fine-Tuning via Circular Convolution|$C^3A$|ACL|[[PDF](https://arxiv.org/pdf/2407.19342)]|
|2025|LoRMA: Low-Rank Multiplicative Adaptation for LLMs|LoRMA|ACL Findings|[[PDF](https://arxiv.org/pdf/2506.07621)][[CODE](https://github.com/Exploration-Lab/LoRMA)]|
|2025|LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation|LoRA-drop|COLING|[[PDF](https://aclanthology.org/2025.coling-main.371.pdf)]|
|2025|Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment|GOAT|ICML|[[PDF](https://arxiv.org/pdf/2502.16894)][[CODE](https://github.com/Facico/GOAT-PEFT)]|
|2025|LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently|LoRA-One|ICML|[[PDF](https://openreview.net/pdf?id=KwIlvmLDLm)][[CODE](https://github.com/YuanheZ/LoRA-One)]|
|2025|SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity|SparseLoRA|ICML|[[PDF](https://openreview.net/pdf?id=z83rodY0Pw)]|
|2025|LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits|LowRA|ICML|[[PDF](https://openreview.net/pdf?id=Fm0nDMKBwC)]|
|2025|SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression|SLiM|ICML|[[PDF](https://openreview.net/pdf?id=4UfRP8MopP)]|
|2025|Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape|Flat-LoRA|ICML|[[PDF](https://openreview.net/pdf?id=3Qj3xSwN2I)][[CODE](https://github.com/nblt/Flat-LoRA)]|
|2025|BSLoRA: Enhancing the Parameter Efficiency of LoRA with Intra-Layer and Inter-Layer Sharing|BSLoRA|ICML|[[PDF](https://openreview.net/pdf?id=IXYBuwCOMl)][[CODE](https://github.com/yuhua-zhou/BSLoRA)]|
|2025|SeedLoRA: A Fusion Approach to Efficient LLM Fine-Tuning|SeedLoRA|ICML|[[PDF](https://openreview.net/pdf?id=7QH48TtFZX)][[CODE](https://github.com/NUS-HPC-AI-Lab/SeedLoRA)]|
|2025|Text-to-LoRA: Instant Transformer Adaption|Text-to-LoRA|ICML|[[PDF](https://openreview.net/pdf?id=zWskCdu3QA)][[CODE](https://github.com/SakanaAI/text-to-lora)]|
|2025|LoRA-Gen: Specializing Large Language Model via Online LoRA Generation|LoRA-Gen|ICML|[[PDF](https://openreview.net/pdf?id=oZM5g4IvmS)]|
|2025|RepLoRA: Reparameterizing Low-rank Adaptation via the Perspective of Mixture of Experts|RepLoRA|ICML|[[PDF](https://openreview.net/pdf?id=Sg8ZqQ9J6W)]|
|2025|mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs|mLoRA|VLDB|[[PDF](https://arxiv.org/pdf/2312.02515)]|
|2025|MoRA: High-Rank Updating for Parameter-Efﬁcient Fine-Tuning|MoRA|arXiv|[[PDF](https://arxiv.org/pdf/2405.12130)][[CODE](https://github.com/kongds/MoRA)]|
|2025|Balancing LoRA Performance and Efficiency with Simple Shard Sharing|MiSS|arXiv|[[PDF](https://arxiv.org/pdf/2409.15371)][[CODE](https://github.com/JL-er/MiSS)]|

### PEFT with other techs
|Year|Paper Title|Proposed Method|Venue and Year|Materials|
|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|
|2022|FedPara: Low-rank Hadamard Product for Communication-Efficient Federated Learning|FedPara|ICLR|[[PDF](https://openreview.net/pdf?id=d71n4ftoCBy)][[CODE](https://github.com/South-hw/FedPara_ICLR22)]|
|2023|Efficient Federated Learning for Modern NLP|AutoFedNLP|MobiCom|[[PUB](https://dl.acm.org/doi/abs/10.1145/3570361.3592505)][[PDF](https://www.caidongqi.com/pdf/AutoFedNLP.pdf)][[CODE](https://github.com/UbiquitousLearning/FedAdapter)]|
|2023|FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models|FedPETuning|ACL Findings|[[PUB](https://aclanthology.org/2023.findings-acl.632/)][[CODE](https://github.com/SMILELab-FL/FedPETuning)]|
|2023|FedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models|FedMS|IEEE TMC|[[PDF](https://arxiv.org/pdf/2312.15926)]|
|2023|Fine-Tuning Language Models with Just Forward Passes|MeZO|NeurIPS|[[PDF](https://arxiv.org/pdf/2305.17333)][[CODE](https://github.com/princeton-nlp/MeZO)]|
|2023|SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models|SLoRA|NeurIPS|[[PDF](https://openreview.net/pdf?id=06quMTmtRV)]|
|2023|MultiLoRA: Democratizing LoRA for Better Multi-Task Learning|MultiLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2311.11501)]|
|2023|PrivateLoRA For Efficient Privacy Preserving LLM|PrivateLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2311.14030)]|
|2023|pFedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning|pFedLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2310.13283)]|
|2023|SiRA: sparse mixture of low rank adaptation|SiRA|arXiv|[[PDF](https://arxiv.org/pdf/2311.09179)]|
|2024|Mixture-of-Subspaces in Low-Rank Adaptation|MoSLoRA|EMNLP|[[PDF](https://aclanthology.org/2024.emnlp-main.450.pdf)]|
|2024|Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models|HETLoRA|EMNLP|[[PDF](https://aclanthology.org/2024.emnlp-main.717.pdf)]|
|2024|MTLoRA: Low-Rank Adaptation Approach for Efficient Multi-Task Learning|MTLoRA|CVPR|[[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Agiza_MTLoRA_Low-Rank_Adaptation_Approach_for_Efficient_Multi-Task_Learning_CVPR_2024_paper.pdf)]|
|2024|Multi-Task Dense Prediction via Mixture of Low-Rank Experts|MLoRE|CVPR|[[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Multi-Task_Dense_Prediction_via_Mixture_of_Low-Rank_Experts_CVPR_2024_paper.pdf)]|
|2024|LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition|LoraHub|COLM|[[PDF](https://arxiv.org/pdf/2307.13269)][[CODE](https://github.com/sail-sg/lorahub)]|
|2024|FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations|FLoRA|NeurIPS|[[PDF](https://openreview.net/pdf?id=TcCorXxNJQ)][[CODE](https://github.com/ATP-1010/FederatedLLM)]|
|2024|Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources|FlexLoRA|NeurIPS|[[PDF](https://arxiv.org/pdf/2402.11505)]|
|2024|Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning|MoV, MoLoRA|ICLR|[[PDF](https://openreview.net/pdf?id=EvDeiLv7qc)][[CODE](https://github.com/Cohere-Labs-Community/parameter-efficient-moe)]|
|2024|Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE|Octavius|ICLR|[[PDF](https://openreview.net/pdf?id=rTDyN8yajn)]|
|2024|Batched Low-Rank Adaptation of Foundation Models|Fast LoRA|ICLR|[[PDF](https://openreview.net/pdf?id=w4abltTZ2f)]|
|2024|Improving LoRA in Privacy-preserving Federated Learning|FFA-LoRA|ICLR|[[PDF](https://openreview.net/pdf?id=NLPzL6HWNl)]|
|2024|FwdLLM: Efficient Federated Finetuning of Large Language Models with Perturbed Inferences|FwdLLM|USENIX ATC|[[PUB](https://www.usenix.org/conference/atc24/presentation/xu-mengwei)]|
|2024|Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark|ZO-LLM|ICML|[[PUB](https://sites.google.com/view/zo-tutorial-aaai-2024/)][[PDF](https://arxiv.org/pdf/2402.11592)][[CODE](https://github.com/ZO-Bench/ZO-LLM)]|
|2024|Mixture-of-LoRAs: An Efficient Multitask Tuning Method for Large Language Models|MoA|LREC-COLING|[[PDF](https://aclanthology.org/2024.lrec-main.994.pdf)]|
|2024|Higher layers need more LoRA experts|MoLA|arXiv|[[PDF](https://arxiv.org/pdf/2402.08562)][[CODE](https://github.com/GCYZSL/MoLA)]|
|2024|LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs|LLaVA-MoLE|arXiv|[[PDF](https://arxiv.org/pdf/2401.16160)]|
|2024|FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition|FeDeRA|arXiv|[[PDF](https://arxiv.org/pdf/2404.18848)]|
|2025|Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning|DP-LoRA|ACM TMIS|[[PDF](https://dl.acm.org/doi/pdf/10.1145/3682068)]|
|2025|Federated Residual Low-Rank Adaptation of Large Language Models|FRLoRA|ICLR|[[PDF](https://openreview.net/pdf?id=e0rQRMUhs7)]|
|2025|A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models|MoELoRA_Riemannian|ICML|[[PDF](https://openreview.net/pdf?id=yqyEUcGreT)][[CODE](https://github.com/THUDM/MoELoRA_Riemannian)]|
|2025|IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models|IntLoRA|ICML|[[PDF](https://openreview.net/pdf?id=K3QoqobsiR)][[CODE](https://github.com/csguoh/IntLoRA)]|
### Libraries
* AdapterHub:
  * Address: https://adapterhub.ml/
  * Description: A Unified Library for Parameter-Efficient and Modular Transfer Learning
* PEFT:
  * Address: https://huggingface.co/docs/peft/index
  * Description: State-of-the-art Parameter-Efficient Fine-Tuning.
* LLM-Adapters:
  * Address: https://github.com/AGI-Edgerunners/LLM-Adapters
  * Description: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models
* LLaMA-Factory:
  * Address: https://github.com/hiyouga/LLaMA-Factory
  * Description: Easily fine-tune 100+ large language models with zero-code CLI and Web UI
<!--## Star History

<a href="https://github.com/XiaoshuangJi/Awesome-PEFT&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=XiaoshuangJi/Awesome-PEFT&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=XiaoshuangJi/Awesome-PEFT&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=XiaoshuangJi/Awesome-PEFT&type=Date" />
  </picture>
</a>-->
