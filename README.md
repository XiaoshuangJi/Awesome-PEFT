# Awesome-PEFT
### Papers
|Paper Title|Proposed Method|Venue and Year|Materials|
|------------------------------|------------------------------|------------------------------|------------------------------|
|Parameter-Efficient Transfer Learning for NLP|Adapter tuning|ICML 2019|[[PUB](https://proceedings.mlr.press/v97/houlsby19a.html)][[CODE](https://github.com/google-research/adapter-bert)]|
|Simple, Scalable Adaptation for Neural Machine Translation|Single Adapter|EMNLP 2019|[[PUB](https://aclanthology.org/D19-1165/)]|
|Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning|VLM|EMNLP 2020 Findings|[[PUB](https://aclanthology.org/2020.findings-emnlp.41/)]|
|AdapterFusion: Non-destructive task composition for transfer learning|AdapterFusion|EACL 2021|[[PUB](https://aclanthology.org/2021.eacl-main.39/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter)]|
|Parameter-Efficient Transfer Learning with Diff Pruning|DiffPruning|ACL 2021|[[PUB](https://aclanthology.org/2021.acl-long.378/)]|
|Prefix-Tuning: Optimizing Continuous Prompts for Generation|Prefix tuning|ACL 2021|[[PDF](https://xiangli1999.github.io/pdf/prefix_tuning.pdf)][[CODE](https://github.com/XiangLi1999/PrefixTuning)]|
|Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning|Intrinsic SAID|ACL 2021|[[PDF](https://aclanthology.org/2021.acl-long.568.pdf)]|
|AdapterDrop: On the efficiency of adapters in transformers|AdapterDrop|EMNLP 2021|[[PUB](https://aclanthology.org/2021.emnlp-main.626/)]|
|The Power of Scale for Parameter-Efficient Prompt Tuning| Prompt tuning|EMNLP 2021|[[PUB](https://aclanthology.org/2021.emnlp-main.243/)]|
|Compacter: Efficient low-rank hypercomplex adapter layers|Compacter|NeurIPS 2021|[[PDF](https://proceedings.neurips.cc/paper/2021/file/081be9fdff07f3bc808f935906ef70c0-Paper.pdf)][[CODE](https://github.com/rabeehk/compacter)]|
|Training Neural Networks with Fixed Sparse Masks|Fish-Mask|NeurIPS 2021|[[PDF](https://proceedings.neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf)][[CODE](https://github.com/varunnair18/FISH)]|
|Towards a Unified View of Parameter-Efficient Transfer Learning|MAM Adapter, ParallelAdapter|ICLR 2022|[[PDF](https://xuezhemax.github.io/assets/publications/pdfs/iclr2022_towards.pdf)][[CODE](https://github.com/jxhe/unify-parameter-efficient-tuning)]|
|UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning|UniPELT|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-long.433/)]|
|P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks|P-tuning v2|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-short.8/)]|
|BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models|BitFit|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-short.1/)]|
|FedPara: Low-rank Hadamard Product for Communication-Efficient Federated Learning|FedPara|ICLR 2022|[[PDF](https://openreview.net/pdf?id=d71n4ftoCBy)][[CODE](https://github.com/South-hw/FedPara_ICLR22)]|
|LoRA: Low-Rank Adaptation of Large Language Models|LoRA|ICLR 2022|[[PDF](https://openreview.net/pdf?id=nZeVKeeFYf9)][[CODE](https://github.com/microsoft/LoRA)]|
|Efficient Fine-Tuning of BERT Models on the Edge|FAR|ISCAS 2022|[[PUB](https://ieeexplore.ieee.org/abstract/document/9937567)][[PDF](https://arxiv.org/pdf/2205.01541)]|
|SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters|SparseAdapter|EMNLP 2022 Findings|[[PUB](https://aclanthology.org/2022.findings-emnlp.160/)][[CODE](https://github.com/Shwai-He/SparseAdapter)]|
|Adamix: Mixture-of adapter for parameter-efficient tuning of large language models|Adamix|EMNLP 2022|[[PDF](https://www.microsoft.com/en-us/research/uploads/prod/2022/05/Mixture_of_Adapters-628fa6a57efd3.pdf)][[CODE](https://github.com/microsoft/AdaMix)]|
|Exploring universal intrinsic task subspace via prompt tuning|IPT|EMNLP 2022 Finding|[[PDF](https://arxiv.org/pdf/2110.07867)][[CODE](https://github.com/thunlp/Intrinsic-Prompt-Tuning)]|
|Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning|(IA)<SUP>3<SUP/>|NeurIPS 2022|[[PUB](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html)][[CODE](https://github.com/r-three/t-few)]|
|LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning|LST|NeurIPS 2022|[[PDF](https://proceedings.neurips.cc/paper_files/paper/2022/file/54801e196796134a2b0ae5e8adef502f-Paper-Conference.pdf)]|
|Sparse Low-rank Adaptation of Pre-trained Language Models|SoRA|EMNLP 2023|[[PDF](https://aclanthology.org/2023.emnlp-main.252.pdf)|
|DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation|DyLoRA|EACL 2023|[[PDF](https://aclanthology.org/2023.eacl-main.239.pdf)][[CODE](https://github.com/huawei-noah/Efficient-NLP/tree/main/DyLoRA)]|
|Adaptive budget allocation for parameter-efficient fine-tuning|AdaLoRA|ICLR 2023|[[PDF](https://par.nsf.gov/servlets/purl/10471451)][[CODE](https://github.com/QingruZhang/AdaLoRA)]|
| Parameter-efficient fine-tuning design spaces|S4|ICLR 2023|[[PDF](https://openreview.net/pdf?id=XSRSWxyJIC)]|
|Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning|MPT|ICLR 2023|[[PUB](https://zhenwang9102.github.io/mpt.html)]|
|QLoRA: Efficient Finetuning of Quantized LLMs|QLoRA|NeurIPS 2023|[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html)][[CODE](https://github.com/artidoro/qlora)]|
|Composing Parameter-Efficient Modules with Arithmetic Operation|ControlPE|NeurIPS 2023|[[PDF](https://openreview.net/pdf?id=5r3e27I9Gy)]|
|Krona: Parameter efficient tuning with kronecker adapter|KronA|NeurIPS 2023 Workshop|[[PDF](https://neurips2023-enlsp.github.io/papers/paper_61.pdf)]|
|Controlling Text-to-Image Diffusion by Orthogonal Finetuning|OFT|NeurIPS 2023|[[PUB](https://oft.wyliu.com/)]|
|Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization|Token-Level Adaptation|AICCC 2023|[[PDF](https://dl.acm.org/doi/pdf/10.1145/3639592.3639615)]|
|Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning|SaLoRA|Mathematics 2023|[[PDF](https://mdpi-res.com/d_attachment/mathematics/mathematics-11-04317/article_deploy/mathematics-11-04317.pdf?version=1697536399)]|
|PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching|PILLOW|EMNLP 2023 Industry Track|[[PDF](https://aclanthology.org/2023.emnlp-industry.45.pdf)]|
|Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices|Delta-LoRA|arXiv|[[PDF](https://arxiv.org/pdf/2309.02411)]|
|LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning|LoRA-FA|arXiv|[[PDF](https://arxiv.org/pdf/2308.03303)]|
|A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA|rsLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2312.03732)]|
|IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning|IncreLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2308.12043)][[CODE](https://github.com/FeiyuZhang98/IncreLoRA)]|
|MEFT:Memory-Efficient Fine-Tuning through Sparse Adapter|MEFT|ACL 2024|[[PDF](https://arxiv.org/pdf/2406.04984)][[CODE](https://github.com/CURRENTF/MEFT)]|
|Multimodal Instruction Tuning with Conditional Mixture of LoRA|MixLoRA|ACL 2024|[[PDF](https://aclanthology.org/2024.acl-long.38.pdf)]|
|DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution|DoRA|ACL 2024|[[PDF](https://aclanthology.org/2024.acl-long.626.pdf)][[CODE](https://github.com/MIkumikumi0116/DoRA)]|
|MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning|MELoRA|ACL 2024|[[PDF](https://aclanthology.org/2024.acl-long.168.pdf)]|
|AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models|AFLoRA|ACL 2024|[[PDF](https://aclanthology.org/2024.acl-short.16.pdf)]|
|ResLoRA: Identity Residual Mapping in Low-Rank Adaption|ResLoRA|ACL 2024 Finding|[[PDF](https://aclanthology.org/2024.findings-acl.525.pdf)][[CODE](https://github.com/microsoft/LMOps/tree/main/reslora)]|
|SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning|SIBO|ACL 2024 Finding|[[PDF](https://aclanthology.org/2024.findings-acl.72.pdf)]|
|STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models|STAR|ACL 2024 Finding|[[PDF](https://aclanthology.org/2024.findings-acl.209.pdf)]|
|LoRA Meets Dropout under a Unified Framework|HiddenKey|ACL 2024 Finding|[[PDF](https://aclanthology.org/2024.findings-acl.119.pdf)]|
|LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning|LoRAPrune|ACL 2024 Finding|[[PDF](https://aclanthology.org/2024.findings-acl.178.pdf)][[CODE](https://github.com/aim-uofa/LoRAPrune)]|
|GPT Understands, Too|P-tuning|AI Open 2024|[[PUB](https://www.sciencedirect.com/science/article/pii/S2666651023000141)]|
|Vector-based Random Matrix Adaptation|VeRA|ICLR 2024|[[PUB](https://dkopi.github.io/vera/)]|
|Parameter-Efficient Multi-Task Model Fusion with Partial Linearization|L-LoRA|ICLR 2024|[[PDF](https://openreview.net/pdf?id=iynRvVVAmH)]|
|Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning|LN tuning|ICLR 2024|[[PDF](https://openreview.net/pdf?id=YR3ETaElNK)]|
|Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization|BOFT|ICLR 2024|[[PUB](https://boft.wyliu.com/)]|
|Bayesian Low-rank Adaptation for Large Language Models|Laplace-LoRA|ICLR 2024|[[PDF](https://openreview.net/pdf?id=FJiUyzOF1m)]|
|Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation|LyCORIS|ICLR 2024|[[PDF](https://arxiv.org/pdf/2309.14859)][[CODE](https://github.com/KohakuBlueleaf/LyCORIS)]|
|QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models|QA-LoRA|ICLR 2024|[[PDF](https://openreview.net/pdf?id=WvFoJccpo8)][[CODE](https://github.com/yuhuixu1993/qa-lora)]|
|LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models|LoftQ|ICLR 2024|[[PDF](https://openreview.net/pdf?id=LzPWWPAdY4)][[CODE]](https://github.com/yxli2123/LoftQ)|
|ReLoRA: High-Rank Training Through Low-Rank Updates|ReLoRA|ICLR 2024|[[PDF](https://openreview.net/pdf?id=DLJznSp6X3)][[CODE](https://github.com/guitaricet/relora)]|
|ApiQ: Finetuning of 2-Bit Quantized Large Language Model|ApiQ|EMNLP 2024|[[PDF](https://aclanthology.org/2024.emnlp-main.1168.pdf)][[CODE](https://github.com/BaohaoLiao/ApiQ)]|
|DoRA: Weight-Decomposed Low-Rank Adaptation|DoRA|ICML 2024|[[PDF](https://openreview.net/pdf?id=3d5CIRG1n2)][[CODE](https://github.com/NVlabs/DoRA)]|
|RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation|RoSA|ICML 2024|[[PDF](https://openreview.net/pdf?id=FYvpxyS43U)][[CODE](https://github.com/IST-DASLab/RoSA)]|
|LoRA+: Efficient Low Rank Adaptation of Large Models|LoRA+|ICML 2024|[[PDF](https://openreview.net/pdf?id=NEv8YqBROO)][[CODE](https://github.com/nikhil-ghosh-berkeley/loraplus)]|
|Parameter-Efficient Fine-Tuning with Discrete Fourier Transform|FourierFT|ICML 2024|[[PDF](https://arxiv.org/pdf/2405.03003)][[CODE](https://github.com/Chaos96/fourierft)]|
|Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning|COLA|ICML 2024|[[PDF](https://arxiv.org/pdf/2401.04151)]|
|HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning|HydraLoRA|NeurIPS 2024|[[PDF](https://openreview.net/pdf?id=qEpi8uWX3N)][[CODE](https://github.com/Clin0212/HydraLoRA)]|
|S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity|S2FT|NeurIPS 2024|[[PUB](https://infini-ai-lab.github.io/S2FT-Page/)]|
|LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning|LISA|NeurIPS 2024|[[PDF](https://proceedings.neurips.cc/paper_files/paper/2024/file/687163285b8affc8ee933bdca8e75747-Paper-Conference.pdf)]|
|DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation|DropBP|NeurIPS 2024|[[PDF](https://openreview.net/pdf?id=x4EoTQW7ka)][[CODE](https://github.com/WooSunghyeon/dropbp)]|
|VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks|VB-LoRA|NeurIPS 2024|[[PDF](https://arxiv.org/pdf/2405.15179)][[CODE](https://github.com/leo-yangli/VB-LoRA)]|
|Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation|HRA|NeurIPS 2024|[[PDF](https://arxiv.org/pdf/2405.17484)][[CODE](https://github.com/DaShenZi721/HRA)]|
|PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models|PiSSA|NeurIPS 2024|[[PDF](https://arxiv.org/pdf/2404.02948)][[CODE](https://github.com/GraphPKU/PiSSA)]|
|CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning|CorDA|NeurIPS 2024|[[PDF](https://openreview.net/pdf?id=Gi00NVru6n)][[CODE](https://github.com/iboing/CorDA)]|
|LoRA-GA: Low-Rank Adaptation with Gradient Approximation|LoRA-GA|NeurIPS 2024|[[PDF](https://openreview.net/pdf?id=VaLAWrLHJv)][[CODE](https://github.com/Outsider565/LoRA-GA)]|
|ReFT: Representation Finetuning for Language Models|ReFT|NeurIPS 2024|[[PDF](https://arxiv.org/pdf/2404.03592)][[CODE](https://github.com/stanfordnlp/pyreft)]|
|LoRA-GA: Low-Rank Adaptation with Gradient Approximation|LoRA-GA|NeurIPS 2024|[[PDF](https://proceedings.neurips.cc/paper_files/paper/2024/file/62c4718cc334f6a0a62fb81c4a2095a1-Paper-Conference.pdf)][[CODE](https://github.com/Outsider565/LoRA-GA)]|
|One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation|EVA|NeurIPS 2024 Workshop|[[PDF](https://openreview.net/pdf?id=X6AOzi82oo)]|
|SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors|SVFT|NeurIPS 2024 Workshop|[[PDF](https://openreview.net/pdf?id=DOUskwCqg5)]|
|ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models|ALoRA|NAACL 2024|[[PDF](https://aclanthology.org/2024.naacl-long.35.pdf)]|
|AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning|AutoLoRA|NAACL 2024|[[PDF](https://aclanthology.org/2024.naacl-long.282.pdf)][[CODE](https://github.com/ruz048/AutoLoRA)]|
|X-LoRA: Mixture of low-rank adapter experts, a flexible framework for large language models with applications in protein mechanics and molecular design|X-LoRA|APL Machine Learning 2024|[[PUB](https://pubs.aip.org/aip/aml/article/2/2/026119/3294581/X-LoRA-Mixture-of-low-rank-adapter-experts-a)][[PDF](https://arxiv.org/pdf/2402.07148)]|
|LoRA-SP: streamlined partial parameter adaptation for resource efficient fine-tuning of large language models|LoRA-SP|AMNA 2024|[[PUB](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13171/131711Z/LoRA-SP--streamlined-partial-parameter-adaptation-for-resource-efficient/10.1117/12.3032013.short)]|
|S-LoRA: Serving Thousands of Concurrent LoRA Adapters|S-LoRA|MLSys 2024|[[PDF](https://arxiv.org/pdf/2311.03285)][[CODE](https://github.com/S-LoRA/S-LoRA)]|
|Punica: Multi-Tenant LoRA Serving|Punica|MLSys 2024|[[PDF](https://proceedings.mlsys.org/paper_files/paper/2024/file/054de805fcceb78a201f5e9d53c85908-Paper-Conference.pdf)][[CODE](https://github.com/punica-ai/punica)]|
|BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models|BiLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2403.13037)]|
|LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters|LoRA-XS|arXiv|[[PDF](https://arxiv.org/pdf/2405.17604)][[CODE](https://github.com/MohammadrezaBanaei/LoRA-XS)]|
|CaraServe: CPU-Assisted and Rank-Aware LoRA Serving for Generative LLM Inference|CaraServe|arXiv|[[PDF](https://arxiv.org/pdf/2401.11240)]|
|L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models|L4Q|arXiv|[[PDF](https://arxiv.org/pdf/2402.04902)]|
|ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization|ComPEFT|arXiv|[[PDF](https://arxiv.org/pdf/2311.13171)][[CODE](https://github.com/prateeky2806/compeft)]|
|MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning|MiLoRA|NAACL 2025|[[PDF](https://arxiv.org/pdf/2406.09044)][[CODE](https://github.com/sufenlp/MiLoRA)]|
|HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models|HiRA|ICLR 2025|[[PDF](https://openreview.net/pdf?id=TwJrTz9cRS)][[CODE](https://github.com/hqsiswiliam/hira)]|
|LoRA-Pro: Are Low-Rank Adapters Properly Optimized?|LoRA-Pro|ICLR 2025|[[PDF](https://openreview.net/pdf?id=gTwRMU3lJ5)]|
|LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization|LoRA-RITE|ICLR 2025|[[PDF](https://openreview.net/pdf?id=VpWki1v2P8)]|
|SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning|SD-LoRA|ICLR 2025|[[PDF](https://openreview.net/pdf?id=5U1rlpX68A)][[CODE](https://github.com/WuYichen-97/SD-Lora-CL)]|
|BeamLoRA: Beam-Constraint Low-Rank Adaptation|BeamLoRA|ACL 2025|[[PDF](https://arxiv.org/pdf/2502.13604)]|
|LoRMA: Low-Rank Multiplicative Adaptation for LLMs|LoRMA|ACL 2025 Findings|[[PDF](https://arxiv.org/pdf/2506.07621)][[CODE](https://github.com/Exploration-Lab/LoRMA)]|
|LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation|LoRA-drop|COLING 2025|[[PDF](https://aclanthology.org/2025.coling-main.371.pdf)]|

### PEFT with other techs
|Paper Title|Proposed Method|Venue and Year|Materials|
|------------------------------|------------------------------|------------------------------|------------------------------|
|Efficient Federated Learning for Modern NLP|AutoFedNLP|MobiCom 2023|[[PUB](https://dl.acm.org/doi/abs/10.1145/3570361.3592505)][[PDF](https://www.caidongqi.com/pdf/AutoFedNLP.pdf)][[CODE](https://github.com/UbiquitousLearning/FedAdapter)]|
|FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models|FedPETuning|ACL 2023 Findings|[[PUB](https://aclanthology.org/2023.findings-acl.632/)][[CODE](https://github.com/SMILELab-FL/FedPETuning)]|
|FedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models|FedMS|IEEE TMC 2023|[[PDF](https://arxiv.org/pdf/2312.15926)]|
|Fine-Tuning Language Models with Just Forward Passes|MeZO|NeurIPS 2023|[[PDF](https://arxiv.org/pdf/2305.17333)][[CODE](https://github.com/princeton-nlp/MeZO)]|
|SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models|SLoRA|NeurIPS 2023|[[PDF](https://openreview.net/pdf?id=06quMTmtRV)]|
|MultiLoRA: Democratizing LoRA for Better Multi-Task Learning|MultiLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2311.11501)]|
|PrivateLoRA For Efficient Privacy Preserving LLM|PrivateLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2311.14030)]|
|pFedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning|pFedLoRA|arXiv|[[PDF](https://arxiv.org/pdf/2310.13283)]|
|SiRA: sparse mixture of low rank adaptation|SiRA|arXiv|[[PDF](https://arxiv.org/pdf/2311.09179)]|
|Mixture-of-Subspaces in Low-Rank Adaptation|MoSLoRA|EMNLP 2024|[[PDF](https://aclanthology.org/2024.emnlp-main.450.pdf)]|
|Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models|HETLoRA|EMNLP 2024|[[PDF](https://aclanthology.org/2024.emnlp-main.717.pdf)]|
|MTLoRA: Low-Rank Adaptation Approach for Efficient Multi-Task Learning|MTLoRA|CVPR 2024|[[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Agiza_MTLoRA_Low-Rank_Adaptation_Approach_for_Efficient_Multi-Task_Learning_CVPR_2024_paper.pdf)]|
|Multi-Task Dense Prediction via Mixture of Low-Rank Experts|MLoRE|CVPR 2024|[[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Multi-Task_Dense_Prediction_via_Mixture_of_Low-Rank_Experts_CVPR_2024_paper.pdf)]|
|LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition|LoraHub|COLM 2024|[[PDF](https://arxiv.org/pdf/2307.13269)][[CODE](https://github.com/sail-sg/lorahub)]|
|FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations|FLoRA|NeurIPS 2024|[[PDF](https://openreview.net/pdf?id=TcCorXxNJQ)][[CODE](https://github.com/ATP-1010/FederatedLLM)]|
|Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources|FlexLoRA|NeurIPS 2024|[[PDF](https://arxiv.org/pdf/2402.11505)]|
|Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning|MoV, MoLoRA|ICLR 2024|[[PDF](https://openreview.net/pdf?id=EvDeiLv7qc)][[CODE](https://github.com/Cohere-Labs-Community/parameter-efficient-moe)]|
|Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE|Octavius|ICLR 2024|[[PDF](https://openreview.net/pdf?id=rTDyN8yajn)]|
|Batched Low-Rank Adaptation of Foundation Models|Fast LoRA|ICLR 2024|[[PDF](https://openreview.net/pdf?id=w4abltTZ2f)]|
|Improving LoRA in Privacy-preserving Federated Learning|FFA-LoRA|ICLR 2024|[[PDF](https://openreview.net/pdf?id=NLPzL6HWNl)]|
|FwdLLM: Efficient Federated Finetuning of Large Language Models with Perturbed Inferences|FwdLLM|USENIX ATC 2024|[[PUB](https://www.usenix.org/conference/atc24/presentation/xu-mengwei)]|
|Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark|ZO-LLM|ICML 2024|[[[PUB](https://sites.google.com/view/zo-tutorial-aaai-2024/)][PDF](https://arxiv.org/pdf/2402.11592)][[CODE](https://github.com/ZO-Bench/ZO-LLM)]|
|Mixture-of-LoRAs: An Efficient Multitask Tuning Method for Large Language Models|MoA|LREC-COLING 2024|[[PDF](https://aclanthology.org/2024.lrec-main.994.pdf)]|
|Higher layers need more LoRA experts|MoLA|arXiv|[[PDF](https://arxiv.org/pdf/2402.08562)][[[CODE](https://github.com/GCYZSL/MoLA)]|
|LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs|LLaVA-MoLE|arXiv|[[PDF](https://arxiv.org/pdf/2401.16160)]|
|FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition|FeDeRA|arXiv|[[PDF](https://arxiv.org/pdf/2404.18848)]|
|Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning|DP-LoRA|ACM TMIS 2025|[[PDF](https://dl.acm.org/doi/pdf/10.1145/3682068)]|
### Libraries
* AdapterHub:
  * Address: https://adapterhub.ml/
  * Description: A Unified Library for Parameter-Efficient and Modular Transfer Learning
* PEFT:
  * Address: https://huggingface.co/docs/peft/index
  * Description: State-of-the-art Parameter-Efficient Fine-Tuning.
* LLM-Adapters:
  * Address: https://github.com/AGI-Edgerunners/LLM-Adapters
  * Description: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models
* LLaMA-Factory:
  * Address: https://github.com/hiyouga/LLaMA-Factory
  * Description: Easily fine-tune 100+ large language models with zero-code CLI and Web UI
<!--## Star History

<a href="https://github.com/XiaoshuangJi/Awesome-PEFT&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=XiaoshuangJi/Awesome-PEFT&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=XiaoshuangJi/Awesome-PEFT&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=XiaoshuangJi/Awesome-PEFT&type=Date" />
  </picture>
</a>-->
